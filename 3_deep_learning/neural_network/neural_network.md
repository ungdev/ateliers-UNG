
| ‚öôÔ∏è / üß†      | üè∑Ô∏è / ‚ùåüè∑Ô∏è       | üî¢ / üó≥Ô∏è       |
| -------------- | --------------- | --------------- |
| üß† Deep Learning    | üè∑Ô∏è Supervised or ‚ùåüè∑Ô∏è Unsupervised Learning     | üî¢ R√©gression or üó≥Ô∏è Classification     |


<br>

# R√©seau de neurones
 
Bouh √ßa fait peur, mais vous inqui√©tez pas, c'est en fait assez chill (ou presque).

## 1. Fonctionnement d'un r√©seau neuronal

Construire un r√©seau de neurones, √ßa consiste √† concat√©ner tout un tas de fonctions pour avoir des pr√©dictions vraiment meilleures. Comme si on passait notre donn√©e dans une suite de r√©gressions lin√©aires (on fait pas √ßa avec l'algorithme pr√©c√©dent de r√©gression lin√©aire parce que c'est pas opti et que √ßa marche juste pas, mais √ßa vous donne une id√©e du principe).

Pour √ßa, on utilise des neurones qui sont dispos√©s sur des couches diff√©rentes.

<img src=img/cat_pred.png>

<br>
- Sur la **couche d'entr√©e** (input layer), on va donner au r√©seau une d√©composition de notre donn√©e d'entr√©e, si n√©cessaire. Si on veut pr√©dire l'objet sur une image, on lui passe une image, qui va √™tre d√©compos√©e en niveaux de rouge, de bleu, de vert... par exemple, et chacune de ces informations sera trait√©e par un neurone d'entr√©e diff√©rent.
- Ce sont les **couches cach√©es** (hidden layers) qui vont faire 99,9% du travail. Ce sont ces neurones-l√† qui vont faire les calculs et ajouter des couches de complexit√© aux mod√®les. Il peut y avoir une, deux, dix couches de neurones cach√©es, en fonction des performances recherch√©es.
- Enfin, la **couche de sortie** (output layer) va simplement renvoyer le r√©sultat trouv√© par le r√©seau. Dans l'exemple ci-dessus, le r√©seau d√©tecte √† 62% que l'image est un chien et 38% que c'est un chat, donc le r√©seau de neurones dira que c'est un chien (dommage).

### Mais dis-moi Jamy, qu'est-ce qu'un neurone ?
Un neurone, c'est une fonction lin√©aire qui va faire un simple calcul sur nos donn√©es. On va l'accoler √† une fonction d'activation, qui va casser la lin√©arit√© du r√©seau (sinon, il servirait pas √† grand chose) et donner une sortie pr√©cise √† notre neurone. Cette sortie est ensuite utilis√©e par d'autres neurones de la couche suivante, et ainsi de suite. C'est le "maillage" que vous voyez sur l'image : ici, tous les neurones d'une couche sont tous li√©s √† ceux de la couche suivante.

### Mais dis-moi Jamy, qu'est-ce qu'une fonction d'activation ?
Il en existe tout un tas, mais voici quelques exemples :

<img src=img/activation_functions.png>

Vous retrouvez entre autres, la fonction `seuil` (Perceptron), `ReLU`, qui est une sorte de seuil, la fonction `tangente hyperbolique`, `sigmo√Øde`...


L√† tout de suite, ca fait pas mal de concepts abstraits, mais dites-vous juste qu'un r√©seau de neurones, √ßa se contente de :
-> r√©cup√©rer des donn√©es d'entr√©e
-> traiter ces donn√©es avec une myriade de calculs simples
-> donner une sortie qui √©value le degr√© de confiance du r√©seau par rapport √† sa r√©ponse

Si on simplifie, c'est juste une tr√®s tr√®s grosse fonction math√©matique avec des param√®tres √† trouver.
Ces param√®tres justement, c'est ce qu'on appelle des **poids** et des **biais**. Sur les liens qu'il existe entre les neurones (tous les traits qui connectent les neurones entre eux sur le sch√©ma), on peut coefficienter les valeurs, par exemple, un poids de 0.1 sur un lien va diminuer grandement l'impact du r√©sultat du neurone pr√©c√©dent dans le calcul du neurone suivant.

<img src=img/cat_pred_weights.png>

Sur le neurone B1, on va effectuer un calcul en prenant `0.7*(A1)`, `0.4*(A2)`, `-1.3*(A3)` et `1.4*(A4)` comme entr√©e.
Par exemple, le neurone B1 peut sommer les entr√©es, et ajouter un biais, soit : `B1 = 0.7*(A1) + 0.4*(A2) + -1.3*(A3) + 1.4*(A4) + biais`. On utilise une fonction d'activation sur ce neurone, et paf, on a le r√©sultat √† envoyer aux prochains neurones.

Si vous n'avez pas tout saisi, ne vous en faites pas, voici un simple exercice pour mieux comprendre comment √ßa fonctionne. Calculez la sortie du neurone vert.

<img src=img/nn_exercise.png>

<details><summary>üí° R√©ponse :</summary>
La r√©ponse est <strong>0</strong> !
La somme des 3 neurones bleus, avec les poids respectifs, donne -4.3. Avec le biais, on a -1.3. Avec la fonction d'activation, on a bien max(0, -1.3) = 0.
</details>

<br>

Bon tout √ßa c'est sympa...mais fort heureusement, vu qu'ici on parle d'intelligence artificielle, c'est la machine qui va faire tous ces calculs, d√©cider des poids √† mettre, ou de certaines fonctions √† utiliser ! Dans le jargon, on appelle ce proc√©d√© de calcul de l'ordinateur de la **forward propagation** ou du **feed-forward**.

### Mais dis-moi Jamy, comment est-ce qu'on fait pour d√©terminer les param√®tres ?
On ne va pas rentrer dans les d√©tails math√©matiques, mais globalement, le but de l'algorithme va √™tre de minimiser la **fonction de co√ªt** associ√©e au r√©seau. La fonction de co√ªt prend en param√®tres les poids et biais, afin d'en sortir une grosse fonction dont il faut trouver le minimum. C'est comme l√¢cher une bille, et trouver le creux le plus bas.

<img src=img/cost_function_3d.png>

Pour trouver ce minimum, on utilise un proc√©d√© math√©matique un peu long et p√©nible qui s'appelle la **descente de gradient**. Concr√®tement, on calcule it√©rativement le gradient *(des d√©riv√©es de fonctions √† plusieurs variables, si vous avez pas encore fait MT04 ou PHYS11)* en descendant la pente jusqu'√† trouver le minimum. S'il est local c'est bien, global c'est mieux, mais c'est pas toujours √©vident (Eviden).

<img src=img/gradient_descent.png>

Beaucoup beaucoup de blabla, mais maintenant, on peut passer √† la pratique !

## 2. Pratique du r√©seau de neurones guid√©e

Nous n'allons pas expliquer √©tape par √©tape l'int√©gration du r√©seau de neurones. Toutefois, on vous donne l'enti√®ret√© du code comment√©, pour que vous compreniez, le fonctionnement pas-√†-pas du r√©seau ! Apr√®s cela, vous pourrez appliquer un r√©seau de neurones √† un autre jeu de donn√©es, sur des images... Donc restez attentifs üòâ

```py
# Libraries import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, LeakyReLU, ELU
from keras.optimizers import Adam

# Load the dataset
data = pd.read_csv('kc_house_data.csv')

X = data.drop(['price'], axis=1)   # We use every feature, except the price we're trying to predict
y = data['price']

# Standardize the data
scale = StandardScaler()   # Initialize the scaler
scale.fit(X)   # Fit it to the data
scaled_X = scale.transform(X)   # Transform the data according to the fitted scaler

# Separate the data as test and train data
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)

# Define the model
model = Sequential()   # Initialize the structure of the neural network
model.add(Dense(10, input_dim=X_train.shape[1]))   # Add a layer of 10 neurons, the input layer is implicit with X_train.shape[1] neurons
model.add(LeakyReLU(alpha=0.1))   # Tell the model that those first 10 neurons are of activation function LeakyReLU. alpha = slope for negative values (see LeakyReLU graph)
model.add(Dense(32))   # Add a layer of 32 neurons, input shape is inferred from last layer
model.add(ELU(alpha=1.0))   # Add the ELU activation function for those 32 neurons
model.add(Dense(64))   # Add a layer of 64 neurons
model.add(LeakyReLU(alpha=0.1))   # Add the LeakyReLU activation function for those 64 neurons
model.add(Dense(1, activation='linear'))   # Use a linear activation function (= no activation) for output layer. 1 neuron since we want 1 value

optimizer = Adam(learning_rate=0.003)   # Optimizer defines the process to diminish the cost function. Here, we use the Adam optimizer, an already existing algorithm

# Compile the model
model.compile(loss='mean_squared_error', optimizer=optimizer)

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Predict house prices
y_pred = model.predict(X_test)[:, 0]

rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
print('Root Mean Squared Error:', rmse)

# Plot the predicted values against the actual values using a linear regression model
plt.scatter(y_pred, y_test)
# Plot a line x = y
plt.plot([0, max(max(y_test), max(y_pred))], [0, max(max(y_test), max(y_pred))], color='red')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Predicted Price vs Actual Price')
plt.show()
```

## 3. Pratique du r√©seau de neurones autonome

On va maintenant vous laisser pratiquer de vous-m√™me les r√©seaux de neurones, en repartant de la base pr√©c√©dente, pour travailler sur le jeu de donn√©es MNIST, qui contient 70 000 images de chiffres √©crits √† la main sur des images de 28x28px.

<img src=img/MNIST.jpg>

Ci-dessous une liste d'indications pour vous aider sur diff√©rents points. Le but √©tant de pratiquer, essayez d'en utiliser le moins possible et n'h√©sitez pas √† nous demander conseil !

Pour load le dataset, installez keras et utilisez ces lignes de code dans un fichier Python :
```
pip install tensorflow keras
```

```py
from keras.datasets import mnist

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```
<br>
‚ö†Ô∏è Pour limiter les temps de calcul, ne s√©lectionnez que les 1 000 premiers √©l√©ments de <code>X_train</code> et <code>y_train</code>, comme suit :

```py
X_train = X_train[:1000]
y_train = y_train[:1000]
```

<br>
<details><summary><b>üí° Indication 1 : librairies conseill√©es</b></summary>
Pour commencer √† doucement vous aiguiller, voici toutes les librairies utilis√©es :

````python
from keras.datasets import mnist   # Dataset
from keras.models import Sequential
from keras.layers import Dense, LeakyReLU, ReLU
from keras.optimizers import Adam
import numpy as np
from keras.utils import to_categorical   # To encode data in the right format
````
</details>
<br>
<details><summary><b>üí° Indication 2a : traitement des donn√©es</b></summary>
Vous pouvez afficher les images en utilisant la fonction <code>plt.imshow(X_train[i], cmap="gray")</code> suivie de <code>plt.show()</code>. Le nombre correspondant est contenu dans <code>y_train[i]</code>.

Il faut savoir que les mod√®les de deep learning vont, la majorit√© du temps, demander en entr√©e des vecteurs de donn√©es plut√¥t que des matrices. Hors, on travaille ici avec des images de 28x28px, encod√©es dans des matrices de 28x28 ! Vous pouvez observer le rendu via le fonction <code>print(X_train[i])</code>.
Il faut donc transformer les matrices en vecteurs...

<details><summary><b>üí° Indication 2b : transformation des matrices</b></summary>
On utilise le code suivant pour paser de matrices 28x28 √† un vecteur de 284 √©l√©ments :

````python
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])
````

Concr√®tement, on r√©assigne <code>X_train</code> et <code>X_test</code> (les matrices d'images) un vecteur. La fonction <code>reshape()</code> de <code>numpy</code> prend en premier argument le nombre de lignes et en second argument, le nombre de colonnes. Ici, on veut un ensemble de vecteurs de 28*28 = 784 √©l√©ments, donc c'est ce qu'on donne comme second argument. Le premier argument donne juste la taille du jeu de donn√©es. <code>X_train</code> est alors une matrice de taille 60 000x784 ! 60 000 entr√©es d'entra√Ænement r√©parties sur 60 000 lignes.
Vous pouvez vous demander pourquoi on a transform√© <code>X_train</code> en matrice alors qu'on voulait un vecteur...mais puisqu'on a tout un dataset, on a n√©cessairement une multitude d'entr√©es r√©parties dans une matrice. Mais on est ainsi pass√©s d'une matrice...de matrices, √† une matrice de vecteurs, et l'objectif est rempli.
</details>

<details><summary><b>üí° Indication 2c : encodage des donn√©es</b></summary>
Un r√©seau de neurones est un algorithme de <b>classifiction</b> ! C'est-√†-dire qu'il donne en sortie une probabilit√© qu'un objet appartienne √† une cat√©gorie. Ici, dans <code>y_train</code> et <code>y_test</code>, on a des √©l√©ments entiers de 0 √† 9 qui repr√©sentent le nombre de l'image. Mais notre r√©seau de neurones, lui, ne comprend pas tr√®s bien ce genre de donn√©es. Pour lui simplifier la t√¢che, on lui traduit ces donn√©es dans des cat√©gories simples.

<img src="img/one_hot_encoder.png">

Ce proc√©d√© s'appelle du <code><i>One Hot Encoding</i></code>. Et pour √ßa, on utilise <code>to_categorical()</code> de <code>keras</code>.

````python
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
````
</details>
</details>

<br>

Afin d'am√©liorer les performances, la stabilit√©, et la compatibilit√© des donn√©es, on va toujours chercher √† les translater sur une plage <code>[-1, 1]</code> ou <code>[0, 1]</code>. Par exemple, la fonction d'activation <b><i>sigmo√Øde</i></b> n'accepte que des entr√©es entre 0 et 1 (jusque-l√†, logique). Donc pour mieux faire converger nos mod√®les, pensez √† mapper les donn√©es sur la plage appropri√©e !

<details><summary><b>üí° Indication 3 : mappage des donn√©es</b></summary>
En regardant les donn√©es contenues dans les images, vous voyez que chaque pixel est repr√©sent√© par une valeur allant de 0 √† 255 (les images √©tant en noir et blanc, on a seulement une valeur de luminance et pas de RGB).
En divisant par 255, on obtient donc imm√©diatement des valeurs comprises entre 0 et 1.

````python
X_train = X_train / 255.0
X_test = X_test / 255.0
````
</details>

<br>

Pour vous simplifier le travail, n'h√©sitez pas √† reprendre le r√©seau neuronal pr√©c√©dent, en retirant √©ventuellement une couche cach√©e pour limiter le temps de calcul.
‚ö†Ô∏è Utilisez une fonction d'activation de la couche de sortie adapt√©e ! Pensez au mappage de vos donn√©es...

<br>

<details><summary><b>üí° Indication 4 : d√©finition du mod√®le</b></summary>
Suivant les indications ci-dessus, on a (par exemple) :

````python
model = Sequential()
model.add(Dense(10, input_dim=X_train.shape[1]))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(32))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(10, activation='sigmoid'))

optimizer = Adam(learning_rate=0.003)

model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32)
````
</details>

<br>

On peut √©valuer les performances du mod√®le √† reconnaitre les bons nombres avec une simple m√©trique d'<code>accuracy</code> :

```py
test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)
```

<br>

En fonction de vos param√®tres, on obtiendra facilement un tr√®s bon r√©sultat au-dessus de 80% compte tenu de la simplicit√© du mod√®le et du peu de donn√©es utilis√©es.

<br>

## Bravo ! üéâ
Vous √™tes arriv√©s √† la fin de cet atelier et avez r√©dig√© votre premier r√©seau de neurones. La ma√Ætrise de ces algorithmes vient avec √©norm√©ment de pratique, notamment pour savoir quelles fonctions d'activation utiliser quand et pourquoi.

Si le sujet vous a int√©ress√©, n'h√©sitez pas √† vous documenter sur Internet. Les cours de [Harvard](https://pll.harvard.edu/subject/neural-networks) sont gratuits, tout comme ceux de [Google](https://developers.google.com/machine-learning?hl=fr). Ceux-ci sont de fortes introductions qui mettront √©videmment un plus large pied dans la technique que cet atelier.

Nous restons √† votre disposition pour la moindre question, donc n'h√©sitez pas üòâ