# 1. R√©gression lin√©aire

Une r√©gression lin√©aire, c'est quoi ?
Concr√®tement, c'est juste une fonction affine (f(x) = ax + b) qu'on essaie de faire passer au plus proche de nos points.

Par exemple, sur l'image ci-dessous, on consid√®re qu'on veut repr√©senter le prix d'une maison en fonction de sa surface. On a, en vert, des informations du march√© (une maison √† 5k‚Ç¨ pour 5m2, une maison √† 15k‚Ç¨ pour 20m2...), et notre algorithme va tracer une droite qui approxime au mieux toutes ces valeurs.

<img src=img/linear_reg.png>

Pour calculer la performance du mod√®le, on utilise le calcul des **moindres carr√©s** (somme des r√©sidus au carr√©, soit la diff√©rence entre la valeur r√©elle et pr√©dite). Le but, c'est de **minimiser** cette valeur, et donc minimiser la fonction calculant l'erreur du mod√®le.
Fondamentalement, quand on travaille avec un mod√®le d'IA, le but final est de chercher √† r√©duire au maximum les diff√©rentes m√©triques d'erreur qu'on peut avoir : les moindres carr√©s repr√©sentent une m√©trique parmi tant d'autres.


## 1.1 Traitement des donn√©es (ou presque)

Pour mettre en pratique une r√©gression lin√©aire, on utilise un simple jeu de donn√©es sur le prix des maisons. Ce jeu de donn√©es pr√©sente 19 dimensions, mais toutes les variables ne sont pas importantes √† conserver, car pas forc√©ment fortement corr√©l√©e au prix !
Pour √ßa, on repr√©sente une matrice de corr√©lation, qui calcule le "score" de corr√©lation entre chacune des variables (on utilise le calcul de [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)).

Concr√®tement, plus la case est claire, plus les valeurs sont corr√©l√©es (positivement, ou n√©gativement). On va donc exclure beaucoup de variables, comme `yr_renovated`, `sqft_lot` ou `condition`. On finit avec un jeu de donn√©es √† 4 dimensions (pour l'exemple ici).

Si vous voulez traiter le jeu de donn√©es vous-m√™me, vous pouvez le r√©cup√©rer sur Kaggle :
//todo: lien kaggle

Sinon, voici le jeu de donn√©es pr√©-trait√© :
//todo: lien jeu de donn√©es

## 1.2 Pratique de la r√©gression 

Pour la r√©gression lin√©aire, il faut tout d'abord installer les librairies n√©cessaires. Pour ce faire, ex√©cutez la commande suivante :

```
pip install numpy scikit-learn pandas matplotlib
```

On va ensuite, dans un fichier Python, importer le jeu de donn√©es. Mettez votre `kc_house_data.csv` dans le m√™me dossier que votre fichier Python, qu'on commencera avec le code ci-dessous :

```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load the dataset
data = pd.read_csv('kc_house_data.csv')
```

Maintenant vous allez vite remarquer un probl√®me dans nos donn√©es : un nombre de chambres, √ßa oscille entre 1 et 10 disons...alors que les *pieds* carr√©s, √ßa oscille entre 200 et 4 000. Quand on calcule une fonction, avoir des ordres de grandeur si vari√©s, √ßa n'aide pas !
Pour rem√©dier √† √ßa, on utilise un `Scaler` qui va nous faire la mise √† l'√©chelle des valeurs tout seul, *c'est-y-pas magique* ‚ú®.

Pour ce faire, on utilise ce code :
```py
y = data['price']   # y has the price, the data we try to predict
X = data[['bedrooms', 'bathrooms', 'sqft_living']]   # X has the data from which we try to predict the y data (price in this case)

scale = StandardScaler()   # Initialize the scaler
scale.fit(X)   # Fit it to the data
scaled_X = scale.transform(X)   # Transform the data according to the fitted scaler
```


La partie de mise en place faite, on s'occupe de l'algorithme.
En intelligence artificielle, il faut toujours s√©parer les jeux de donn√©es en deux groupes distincts : les donn√©es utilis√©es pour entra√Æner le mod√®le, et les donn√©es utilis√©es pour tester le mod√®le : pendant sa phase d'apprentissage, on entra√Æne le mod√®le sur g√©n√©ralement **80%** du jeu de donn√©es, pour ensuite l'√©valuer sur **20%** du jeu de donn√©es, qu'il n'a jamais vu auparavant et qu'il n'utilisera pas pour am√©liorer son mod√®le. Cela permet de tester la robustesse, capacit√© √† g√©n√©raliser, du mod√®le face √† des donn√©es nouvelles.

Pour s√©parer nos donn√©es, on utilise la fonction `test_train_split` de la librairie `scikit-learn` qui fait tout pour nous, en utilisant une seed :
```py
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   # test_size = proportion of dataset used for testing ; random_state = seed
```

Ceci √©tant fait, on peut d√©sormais initiliaser le mod√®le et l'entra√Æner... et pour une r√©gession lin√©aire, pas de panique, on a encore des librairies pour nous m√¢cher le travail !
```py
# Instantiate a linear regression model
model = LinearRegression()

# This step lets the code run on its own all the training process. Right after, 'model' can be called anywhere as a pre-trained model
model.fit(X_train, y_train)
```

A pr√©sent... √† vous de jouer ü´µ. Compl√©tez cette ligne en rempla√ßant l'int√©rieur des arguments de `model.predict()`.
```py
# Predict using the trained model
y_pred = model.predict(///)
```

<details><summary>üí° Indice :</summary>
On cherche ici √† stocker les pr√©dictions du mod√®le calcul√©es sur les donn√©es de <strong>test</strong>. Donc, on fait une pr√©diction sur <code>X_test</code> (et non pas <code>y_test</code>, car c'est la "r√©ponse" √† ce qu'on cherche : en donnant <code>X_test</code>, on approxime <code>y_test</code> et on rentre ces pr√©dictions dans <code>y_pred</code>).

On a donc :
```py
y_pred = model.predict(X_test)
```
</details>

Etape importante ! Il faut maintenant calculer une m√©trique d'erreur, de performance, du mod√®le. Comme pr√©sent√© en <a href='#1-r√©gression-lin√©aire'>#1. R√©gression lin√©aire</a>, on va ici utiliser la m√©thode des moindres carr√©s (et plus pr√©cis√©ment, sa racine carr√©e) pour estimer l'erreur du mod√®le :
```py
# Calculate the root mean squared error
rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
print('Root Mean Squared Error:', rmse)
```
On obtient normalement une RMSE d'environ **272 466**. La RMSE d√©pendra toujours des donn√©es utilis√©es (ici, on travaille sur des prix avec de tr√®s grands ordres de grandeur).
Spoiler Alert : c'est quand m√™me pas tr√®s bon comme score. Mais √ßa s'explique par plusieurs facteurs :
- on utilise que 3 dimensions. C'est peu, tr√®s peu.
- la relation n'est pas forc√©ment lin√©aire
- le jeu de donn√©es est biais√© (la r√©partition des prix des maisons est in√©gale)

Ca n'emp√™che qu'on peut tout de m√™me repr√©senter visuellement la performance de notre mod√®le. On a d√©cid√© de le repr√©senter autour d'une ligne droite : plus les points sont proches de la ligne rouge (X = Y), plus les pr√©dictions sont bonnes.
```py
# Plot the predicted values against the actual values using a linear regression model
plt.scatter(y_pred, y_test)
# Plot a line x = y
plt.plot([0, max(max(y_test), max(y_pred))], [0, max(max(y_test), max(y_pred))], color='red')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Predicted Price vs Actual Price')
plt.show()
```

Si vous le souhaitez, pour am√©liorer les performances du mod√®le, vous pouvez tester avec + de variables. On a ainsi rapidement eu 22% d'erreur en moins...
Le code final est disponible sur le GitHub.


# 2. R√©seau de neurones
 
Bouh √ßa fait peur, mais vous inqui√©tez pas, c'est en fait assez chill (ou presque).
Construire un r√©seau de neurones, √ßa consiste √† concat√©ner tout un tas de fonctions pour avoir des pr√©dictions vraiment meilleures. Comme si on passait notre donn√©e dans une suite de r√©gressions lin√©aires (on fait pas √ßa avec l'algorithme pr√©c√©dent de r√©gression lin√©aire parce que c'est pas opti et que √ßa marche juste pas, mais √ßa vous donne une id√©e du principe).

Pour √ßa, on utilise des neurones qui sont dispos√©es sur des couches diff√©rentes.

<img src=img/cat_pred.png>

- Sur la **couche d'entr√©e** (input layer), on va donner au r√©seau une d√©composition de notre donn√©e d'entr√©e, si n√©cessaire. Si on veut pr√©dire l'objet sur une image, on lui passe une image, qui va √™tre d√©compos√©e en niveaux de rouge, de bleu, de vert... par exemple, et chacune de ces informations sera trait√©e par un neurone d'entr√©e diff√©rent.
- Ce sont les **couches cach√©es** (hidden layers) qui vont faire 99,9% du travail. Ce sont ces neurones-l√† qui vont faire les calculs et ajouter des couches de complexit√© aux mod√®les. Il peut y avoir une, deux, dix couches de neurones cach√©es, en fonction des performances recherch√©es.
- Enfin, la **couche de sortie** (output layer) va simplement renvoyer le r√©sultat trouv√© par le r√©seau. Dans l'exemple ci-dessus, le r√©seau d√©tecte √† 62% que l'image est un chien et 38% que c'est un chat, donc le r√©seau de neurones dira que c'est un chien (dommage).

###### Mais dis-moi Jamy, qu'est-ce qu'un neurone ?
Un neurone, c'est une fonction lin√©aire qui va faire un simple calcul sur nos donn√©es. On va l'accoler √† une fonction d'activation, qui va casser la lin√©arit√© du r√©seau (sinon, il servirait pas √† grand chose) et donner une sortie pr√©cise √† notre neurone. Cette sortie est ensuite utilis√©e par d'autres neurones de la couche suivante, et ainsi de suite. C'est le "maillage" que vous voyez sur l'image : ici, tous les neurones d'une couche sont tous li√©s √† ceux de la couche suivante.

###### Mais dis-moi Jamy, qu'est-ce qu'une fonction d'activation ?
Il en existe tout un tas, mais voici quelques exemples :

<img src=img/activation_functions.png>

Vous retrouvez entre autres, la fonction `seuil` (Perceptron), `ReLU`, qui est une sorte de seuil, la fonction `tangente hyperbolique`, `sigmo√Øde`...


L√† tout de suite, ca fait pas mal de concepts abstraits, mais dites-vous juste qu'un r√©seau de neurones, √ßa se contente de :
-> r√©cup√©rer des donn√©es d'entr√©e
-> traiter ces donn√©es avec une myriade de calculs simples
-> donner une sortie qui √©value le degr√© de confiance du r√©seau par rapport √† sa r√©ponse

Si on simplifie, c'est juste une tr√®s tr√®s grosse fonction math√©matique avec des param√®tres √† trouver.
Ces param√®tres justement, c'est ce qu'on appelle des **poids** et des **biais**. Sur les liens qu'il existe entre les neurones (tous les traits qui connectent les neurones entre eux sur le sch√©ma), on peut coefficienter les valeurs, par exemple, un poids de 0.1 sur un lien va diminuer grandement l'impact du r√©sultat du neurone pr√©c√©dent dans le calcul du neurone suivant.

<img src=img/cat_pred_weights.png>

Sur le neurone B1, on va effectuer un calcul en prenant `0.7*(A1)`, `0.4*(A2)`, `-1.3*(A3)` et `1.4*(A4)` comme entr√©e.
Par exemple, le neurone B1 peut sommer les entr√©es, et ajouter un biais, soit : `B1 = 0.7*(A1) + 0.4*(A2) + -1.3*(A3) + 1.4*(A4) + biais`. On utilise une fonction d'activation sur ce neurone, et paf, on a le r√©sultat √† envoyer aux prochains neurones.

Si vous n'avez pas tout saisi, ne vous en faites pas, voici un simple exercice pour mieux comprendre comment √ßa fonctionne. Calculez la sortie du neurone vert.

<img src=img/nn_exercise.png>

<details><summary>üí° R√©ponse :</summary>
La r√©ponse est <strong>0</strong> !
La somme des 3 neurones bleus, avec les poids respectifs, donne -4.3. Avec le biais, on a -1.3. Avec la fonction d'activation, on a bien max(0, -1.3) = 0.
</details>

<br>

Bon tout √ßa c'est sympa...mais fort heureusement, vu qu'ici on parle d'intelligence artificielle, c'est la machine qui va faire tous ces calculs, d√©cider des poids √† mettre, ou de certaines fonctions √† utiliser ! Dans le jargon, on appelle ce proc√©d√© de calcul de l'ordinateur de la **forward propagation** ou du **feed-forward**.

###### Mais dis-moi Jamy, comment est-ce qu'on fait pour d√©terminer les param√®tres ?
On ne va pas rentrer dans les d√©tails math√©matiques, mais globalement, le but de l'algorithme va √™tre de minimiser la **fonction de co√ªt** associ√©e au r√©seau. La fonction de co√ªt prend en param√®tres les poids et biais, afin d'en sortir une grosse fonction dont il faut trouver le minimum. C'est comme l√¢cher une bille, et trouver le creux le plus bas.

<img src=img/cost_function_3d.png>

Pour trouver ce minimum, on utilise un proc√©d√© math√©matique un peu long et p√©nible qui s'appelle la **descente de gradient**. Concr√®tement, on calcule it√©rativement le gradient *(des d√©riv√©es de fonctions √† plusieurs variables, si vous avez pas encore fait MT04 ou PHYS11)* en descendant la pente jusqu'√† trouver le minimum. S'il est local c'est bien, global c'est mieux, mais c'est pas toujours √©vident (Eviden).

<img src=img/gradient_descent.png>

Beaucoup beaucoup de blabla, mais maintenant, on peut passer √† la pratique !

## 2.1 Pratique du r√©seau de neurones guid√©e

Nous n'allons pas expliquer √©tape par √©tape l'int√©gration du r√©seau de neurones. Toutefois, on vous donne l'enti√®ret√© du code comment√©, pour que vous compreniez, le fonctionnement pas-√†-pas du r√©seau ! Apr√®s cela, vous pourrez appliquer un r√©seau de neurones √† un autre jeu de donn√©es, sur des images... Donc restez attentifs üòâ

```py
# Libraries import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, LeakyReLU, ELU
from keras.optimizers import Adam

# Load the dataset
data = pd.read_csv('kc_house_data.csv')

X = data.drop(['price'], axis=1)   # We use every feature, except the price we're trying to predict
y = data['price']

# Standardize the data
scale = StandardScaler()   # Initialize the scaler
scale.fit(X)   # Fit it to the data
scaled_X = scale.transform(X)   # Transform the data according to the fitted scaler

# Separate the data as test and train data
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)

# Define the model
model = Sequential()   # Initialize the structure of the neural network
model.add(Dense(10, input_dim=X_train.shape[1]))   # Add a layer of 10 neurons, the input layer is implicit with X_train.shape[1] neurons
model.add(LeakyReLU(alpha=0.1))   # Tell the model that those first 10 neurons are of activation function LeakyReLU. alpha = slope for negative values (see LeakyReLU graph)
model.add(Dense(32))   # Add a layer of 32 neurons, input shape is inferred from last layer
model.add(ELU(alpha=1.0))   # Add the ELU activation function for those 32 neurons
model.add(Dense(64))   # Add a layer of 64 neurons
model.add(LeakyReLU(alpha=0.1))   # Add the LeakyReLU activation function for those 64 neurons
model.add(Dense(1, activation='linear'))   # Use a linear activation function (= no activation) for output layer. 1 neuron since we want 1 value

optimizer = Adam(learning_rate=0.003)   # Optimizer defines the process to diminish the cost function. Here, we use the Adam optimizer, an already existing algorithm

# Compile the model
model.compile(loss='mean_squared_error', optimizer=optimizer)

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Predict house prices
y_pred = model.predict(X_test)[:, 0]

rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
print('Root Mean Squared Error:', rmse)

# Plot the predicted values against the actual values using a linear regression model
plt.scatter(y_pred, y_test)
# Plot a line x = y
plt.plot([0, max(max(y_test), max(y_pred))], [0, max(max(y_test), max(y_pred))], color='red')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Predicted Price vs Actual Price')
plt.show()
```

## 2.2 Pratique du r√©seau de neurones autonome

On va maintenant vous laisser pratiquer de vous-m√™me les r√©seaux de neurones, en repartant de la base pr√©c√©dente, pour travailler sur le jeu de donn√©es MNIST, qui contient 70 000 images de chiffres √©crits √† la main sur des images de 28x28px.

<img src=img/MNIST.jpg>

